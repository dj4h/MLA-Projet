{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efec2cdd",
   "metadata": {},
   "source": [
    "# Projet MLA\n",
    "## EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96559494",
   "metadata": {},
   "source": [
    "### MNIST\n",
    "\n",
    "#### Entrainement de 2 modèles simples\n",
    "- Modèle normal, entrainé sur MNIST clean\n",
    "- Modèle *adversarial*, adv. training --> génère adv. examples, mix clean+adv, modèle robuste\n",
    "\n",
    "![](clean_adv_diff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a5e7a5",
   "metadata": {},
   "source": [
    "#### Analyse des résultats\n",
    "- \"Base\" model\n",
    "    - Clean accuracy = 98%\n",
    "    - Adv accuracy = 5-20% (perf détruites par FGSM)\n",
    "- \"adv\" model\n",
    "    - Clean accuracy = 96-98% (légère baisse)\n",
    "    - Adv accuracy = 90-93% (robuste ++)\n",
    "![](comparaison_performances.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14d8e5",
   "metadata": {},
   "source": [
    "## Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec3011b",
   "metadata": {},
   "source": [
    "## Paramètres à faire varier pour étudier la robustesse et la fiabilité\n",
    "|**Paramètre |Effet attendu  / Ce que ça teste** |\n",
    "|----------|----------------------------------|\n",
    "|**Epsilon (FGSM / adversarial attack)**|Intensité de perturbation → robustesse du modèle aux attaques|\n",
    "|**Alpha dans adversarial training**|Poids entre loss clean et loss adversarial → impact sur trade-off clean vs robust accuracy|\n",
    "|**Batch size**|Influence la convergence et stabilité de l’entraînement|\n",
    "|**Learning rate**|Trop grand → modèle instable, trop petit → apprentissage lent|\n",
    "|**Nombre d’époques**|Vérifier si le modèle sur-entraîne ou sous-entraîne|\n",
    "|**Architecture du modèle (nombres de filtres, couches, FC)**|\tTeste si plus complexe = meilleure robustesse ou sur-apprentissage\n",
    "|**Type d’attaque (FGSM, PGD, DeepFool, etc.)**|Teste la robustesse face à différentes perturbations|\n",
    "|**Ajout de bruit aléatoire / transformations**|Data augmentation pour tester généralisation et fiabilité sur images légèrement modifiées|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7878a2af",
   "metadata": {},
   "source": [
    "## Évaluation du modèle sur d'autres base de données\n",
    "\n",
    "### Pourquoi?\n",
    "Dans la 1ère partie on a pu réimplémenter les étapes de constructions du modèle décrit dans l'article. À partir de la base de donnée MNIST de PyTorch, le modèle a pu être entraîné pour établir la méthode d'entraînement adversarial la plus adaptée au traitement de ces données. (/p)\n",
    "Toutefois, nous sommes en doit de nous demander :\n",
    "- Que vaut notre modèle sur une base de donnée alternative?\n",
    "- De quelle manière pourroins nous améliorer les performances de notre modèle en l'entrainant sur d'autres bases de données ?\n",
    "\n",
    "### Bases de données accessibles avec PyTorch\n",
    "- MNIST, sur laquelle est entraîné le modèle\n",
    "- Fashion-MNIST, 28×28 - grayscale\t10 (vêtements)\tTest de généralisation sur données visuelles plus variées que MNIST \n",
    "- CIFAR-10\t32×32, RGB\t10\tImages naturelles, animaux et véhicules\n",
    "- CIFAR-100\t32×32, RGB\t100\tClassification plus fine, test de capacité à gérer plus de classes\n",
    "- SVHN (Street View House Numbers)\t32×32, RGB\t10\tReconnaissance de chiffres dans des contextes réels\n",
    "- MNIST-M\t28×28, RGB\t10\tMNIST modifié avec bruit / background → test de robustesse\n",
    "\n",
    "#### Mention spéciale pour les datasets Kaggle\n",
    "- Custom datasets\tVariable\tVariable\tImages médicales, industrielles, etc.\n",
    "\n",
    "### Critère de choix\n",
    "- **MNIST / Fashion-MNIST**\n",
    "    - Simple, rapide, parfait pour tester un modèle de base\n",
    "- **CIFAR / SVHN**\n",
    "    - Images colorées plus complexes → test de capacité à extraire des features plus fines\n",
    "- **MNIST-M / datasets bruités** \n",
    "    - Tester la robustesse aux perturbations et aux adversarial examples\n",
    "- **Datasets custom**\n",
    "    - Tester la fiabilité dans un contexte réel ou industriel\n",
    "\n",
    "### Adaptation du modèle en fonction des **objectifs** :\n",
    "- Généralisation vs robustesse\n",
    "- Tâche simple vs tâche complexe\n",
    "- Grayscale vs RGB\n",
    "\n",
    "### Quel serait le protocole le plus pertinent pour étudier les limites de notre modèle et améliorer ses performances en l'entrainant sur différentes bases de données?\n",
    "\n",
    "#### Définir les objectifs\n",
    "\n",
    "Avant tout, ce que tu veux mesurer :\n",
    "\n",
    "- Robustesse : comment le modèle résiste aux attaques adversariales ou aux perturbations.\n",
    "- Généralisation : performance sur des données nouvelles / différentes de l’entraînement.\n",
    "- Fiabilité : stabilité des prédictions en présence de bruit, transformations ou variations dans les données.\n",
    "- Performance pure : précision (accuracy), F1-score, etc. sur différentes bases\n",
    "\n",
    "#### Choisir les datasets\n",
    "\n",
    "Sélectionner plusieurs bases de données, avec des caractéristiques variées :\n",
    "\n",
    "- Simples et proches de MNIST : MNIST, Fashion-MNIST → test de base et débogage rapide.\n",
    "- Images colorées / plus complexes : CIFAR-10/100, SVHN → test de généralisation et capacité du modèle à extraire des features complexes.\n",
    "- Variantes bruitées / perturbées : MNIST-M, datasets augmentés → tester robustesse.\n",
    "- Custom / réelles : images médicales, industrielles → tester fiabilité et applicabilité réelle.\n",
    "\n",
    "#### Adapter le modèle\n",
    "\n",
    "- Ajuster entrée et sortie : nombre de canaux (grayscale → RGB), taille d’image, nombre de classes.\n",
    "\n",
    "- Éventuellement modifier l’architecture si le dataset est plus complexe : plus de filtres, couches, dropout, batch normalization.\n",
    "\n",
    "#### Définir les protocoles d’entraînement\n",
    "\n",
    "Pour chaque dataset :\n",
    "1. Mode standard (clean)\n",
    "    - Entraîner le modèle sur données clean.\n",
    "    - Tester sur données clean et adversariales.\n",
    "2. Mode adversarial training\n",
    "    - Générer adversarial examples (FGSM, PGD, etc.)\n",
    "    - Combiner loss clean et loss adversarial (alpha * clean + (1-alpha) * adv)\n",
    "    - Tester sur clean, adversarial, et éventuellement sur bruit/noise.\n",
    "3. Data augmentation / bruit\n",
    "    - Rotation, translation, scaling, bruit gaussien\n",
    "    - Vérifier si la performance et la robustesse s’améliorent.\n",
    "\n",
    "#### Faire varier les bons paramètres\n",
    "Pour chaque dataset, expérimenter avec :\n",
    "\n",
    "|Paramètre|\tObjectif|\n",
    "|-------------|--------------------------------|\n",
    "|Learning rate|Tester stabilité et vitesse d’apprentissage|\n",
    "|Batch size\t|Impact sur convergence et régularisation|\n",
    "|Epsilon (FGSM/PGD)\t|Mesurer robustesse face à perturbations|\n",
    "|Alpha adversarial\t|Trade-off clean vs robust accuracy|\n",
    "|Architecture\t|Nombre de filtres, couches → capacité du modèle|\n",
    "|Nombre d’époques\t|Sur- ou sous-entraînement|\n",
    "|Augmentation / bruit\t|Généralisation et fiabilité|\n",
    "\n",
    "#### Évaluation systématique\n",
    "\n",
    "Pour chaque expérience, mesurer :\n",
    "- Accuracy / Loss sur clean et adversarial\n",
    "- Courbes d’apprentissage (train/test loss vs epochs)\n",
    "- Comparaison graphique des modèles (clean vs adversarial)\n",
    "- Visualisation des adversarial examples et des perturbations\n",
    "- Analyse des erreurs : quelles classes sont les plus fragiles ?\n",
    "\n",
    "#### Comparaison inter-datasets\n",
    "\n",
    "- Observer comment le modèle réagit à différents types de données.\n",
    "- Identifier les datasets ou les conditions qui réduisent les performances.\n",
    "- Tester transfert de connaissances : entraîner sur dataset A, tester sur dataset B → mesure de la généralisation.\n",
    "\n",
    "#### Amélioration itérative\n",
    "\n",
    "À partir des observations :\n",
    "- Ajouter des couches, dropout, batch normalization pour plus de robustesse.\n",
    "- Ajuster alpha, epsilon pour adversarial training optimal.\n",
    "- Ajouter augmentation ou régularisation pour mieux généraliser.\n",
    "- Répéter le protocole sur plusieurs datasets pour confirmer la robustesse et fiabilité."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5375489",
   "metadata": {},
   "source": [
    "### Adaptation du code MNIST à un entrainement multidatasets\n",
    "\n",
    "- Adaptation de fgsm_attack : \n",
    "    - Détacher le tenseur et en créer un nouveau avec grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d344c5a",
   "metadata": {},
   "source": [
    "### Fashion-MNIST\n",
    "##### Entrainement de 2 modèles simples\n",
    "- Modèle normal, entrainé sur Fashion-MNIST clean\n",
    "- Modèle *adversarial*, adv. training --> génère adv. examples, mix clean+adv, modèle robuste\n",
    "\n",
    "![](clean_adv_diff-Fashion-MNIST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dfef8a",
   "metadata": {},
   "source": [
    "#### Analyse des résultats\n",
    "- \"Base\" model\n",
    "    - Clean accuracy = 88%\n",
    "    - Adv accuracy = 0.09-0.26% (perf détruites par FGSM)\n",
    "- \"adv\" model\n",
    "    - Clean accuracy = 85-86% (légère baisse)\n",
    "    - Adv accuracy = 78-85% (robuste ++)\n",
    "![](comparaison_performances-Fashion-MNIST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfbe477",
   "metadata": {},
   "source": [
    "### CIFAR-10\n",
    "##### Entrainement de 2 modèles simples\n",
    "- Modèle normal, entrainé sur CIFAR-10 clean\n",
    "- Modèle *adversarial*, adv. training --> génère adv. examples, mix clean+adv, modèle robuste\n",
    "\n",
    "![](clean_adv_diff_CIFAR-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35020ff4",
   "metadata": {},
   "source": [
    "#### Analyse des résultats\n",
    "- \"Base\" model\n",
    "    - Clean accuracy = 56-58%\n",
    "    - Adv accuracy = 0.23-44% (perf détruites par FGSM)\n",
    "- \"adv\" model\n",
    "    - Clean accuracy = 45% (légère baisse)\n",
    "    - Adv accuracy = 54% (robuste ++)\n",
    "![](comparaison_performances_CIFAR-10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12114f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eaf36d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

## Impact du paramètre ε (epsilon) sur la robustesse du modèle

### Rôle du paramètre ε

Le paramètre ε (epsilon) contrôle l’amplitude maximale de la perturbation appliquée à une image lors de la génération d’exemples adversariaux avec l’attaque **FGSM (Fast Gradient Sign Method)**.

Concrètement, ε définit la norme de la perturbation ajoutée à chaque pixel, selon la relation :

$$
x_{adv} = x + \varepsilon \cdot \text{sign}(\nabla_x L(x, y))
$$

Un ε faible correspond à une perturbation quasi imperceptible, tandis qu’un ε élevé peut dégrader visuellement l’image mais augmente fortement l’efficacité de l’attaque.

---

### Influence de ε sur les modèles entraînés classiquement

Les expériences montrent que les modèles entraînés uniquement sur des données *clean* sont extrêmement sensibles à la valeur de ε.  
Même pour des valeurs faibles, la précision chute drastiquement :

- **MNIST et Fashion-MNIST** :  
  une augmentation modérée de ε suffit à faire passer la précision adversariale de plusieurs dizaines de pourcents à des valeurs proches de zéro.

- **CIFAR-10** :  
  cette sensibilité est encore plus marquée en raison de la complexité visuelle des images et de la présence de textures et de couleurs.

Ce phénomène s’explique par le fait que les modèles standards apprennent des frontières de décision très locales, fortement dépendantes de variations au niveau des pixels, ce qui les rend vulnérables à de petites perturbations dirigées.

---

### Impact de ε dans le cadre de l’entraînement adversarial

Dans le cas de l’entraînement adversarial, ε joue un rôle central dans le compromis entre **robustesse** et **performance sur données propres (clean accuracy)**.

- **ε trop faible**  
  Les perturbations adversariales sont peu informatives. Le modèle apprend peu à se défendre et le gain en robustesse reste limité.

- **ε modéré**  
  Le modèle est confronté à des perturbations réalistes et apprend des représentations plus stables.  
  On observe une augmentation significative de la précision adversariale, avec une baisse limitée de la précision *clean*.

- **ε trop élevé**  
  Les exemples adversariaux deviennent visuellement éloignés des données originales.  
  Le modèle peut alors sur-apprendre à des perturbations irréalistes, ce qui dégrade la performance sur données propres et peut nuire à la généralisation.

Dans nos expériences, les valeurs choisies (ε = 0.1 pour **Fashion-MNIST** et ε = 0.03 pour **CIFAR-10**) correspondent à un compromis empirique permettant d’obtenir une amélioration significative de la robustesse sans perte excessive de précision sur données propres.

---

### Dépendance de ε au dataset

Les résultats montrent que la valeur optimale de ε dépend fortement du type de données :

- **Datasets simples (MNIST, Fashion-MNIST)**  
  Les images étant en niveaux de gris et peu texturées, des valeurs de ε plus élevées peuvent être tolérées sans altérer excessivement la sémantique de l’image.

- **Datasets complexes (CIFAR-10)**  
  Les images RGB contiennent davantage de détails visuels. Une perturbation de même amplitude devient plus perceptible et plus destructrice, nécessitant des valeurs de ε plus faibles.

Cela souligne que ε ne peut pas être choisi de manière universelle et doit être adapté à la résolution, au nombre de canaux et à la complexité visuelle du dataset.

---

### Limites et perspectives

Il est important de noter que la robustesse acquise pour une valeur donnée de ε est généralement spécifique à cette valeur.  
Un modèle entraîné avec un ε fixe peut rester vulnérable à des attaques utilisant des amplitudes différentes ou des méthodes plus puissantes (par exemple **PGD**).

Plusieurs pistes d’amélioration peuvent être envisagées :

- entraînement avec un ε variable (*adversarial curriculum*),
- évaluation sur une plage de valeurs de ε,
- combinaison avec des attaques multi-étapes.

---

### Conclusion

Le paramètre ε constitue un hyperparamètre clé de l’entraînement adversarial.  
Il détermine directement l’intensité des perturbations, influence fortement la robustesse du modèle et conditionne le compromis fondamental entre précision sur données propres et résistance aux attaques adversariales.

Une étude systématique de son impact est donc indispensable pour comprendre les limites et les capacités réelles d’un modèle robuste.
