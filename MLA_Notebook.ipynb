{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efec2cdd",
   "metadata": {},
   "source": [
    "# Projet MLA\n",
    "## EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96559494",
   "metadata": {},
   "source": [
    "### MNIST\n",
    "\n",
    "#### Entrainement de 2 modèles simples\n",
    "- Modèle normal, entrainé sur MNIST clean\n",
    "- Modèle *adversarial*, adv. training --> génère adv. examples, mix clean+adv, modèle robuste\n",
    "\n",
    "![](clean_adv_diff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a5e7a5",
   "metadata": {},
   "source": [
    "#### Analyse des résultats\n",
    "- \"Base\" model\n",
    "    - Clean accuracy = 98%\n",
    "    - Adv accuracy = 5-20% (perf détruites par FGSM)\n",
    "- \"adv\" model\n",
    "    - Clean accuracy = 96-98% (légère baisse)\n",
    "    - Adv accuracy = 90-93% (robuste ++)\n",
    "![](comparaison_performances.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14d8e5",
   "metadata": {},
   "source": [
    "## Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec3011b",
   "metadata": {},
   "source": [
    "## Paramètres à faire varier pour étudier la robustesse et la fiabilité\n",
    "|**Paramètre |Effet attendu  / Ce que ça teste** |\n",
    "|----------|----------------------------------|\n",
    "|**Epsilon (FGSM / adversarial attack)**|Intensité de perturbation → robustesse du modèle aux attaques|\n",
    "|**Alpha dans adversarial training**|Poids entre loss clean et loss adversarial → impact sur trade-off clean vs robust accuracy|\n",
    "|**Batch size**|Influence la convergence et stabilité de l’entraînement|\n",
    "|**Learning rate**|Trop grand → modèle instable, trop petit → apprentissage lent|\n",
    "|**Nombre d’époques**|Vérifier si le modèle sur-entraîne ou sous-entraîne|\n",
    "|**Architecture du modèle (nombres de filtres, couches, FC)**|\tTeste si plus complexe = meilleure robustesse ou sur-apprentissage\n",
    "|**Type d’attaque (FGSM, PGD, DeepFool, etc.)**|Teste la robustesse face à différentes perturbations|\n",
    "|**Ajout de bruit aléatoire / transformations**|Data augmentation pour tester généralisation et fiabilité sur images légèrement modifiées|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7878a2af",
   "metadata": {},
   "source": [
    "## Évaluation du modèle sur d'autres base de données\n",
    "\n",
    "### Pourquoi?\n",
    "Dans la 1ère partie on a pu réimplémenter les étapes de constructions du modèle décrit dans l'article. À partir de la base de donnée MNIST de PyTorch, le modèle a pu être entraîné pour établir la méthode d'entraînement adversarial la plus adaptée au traitement de ces données. (/p)\n",
    "Toutefois, nous sommes en doit de nous demander :\n",
    "- Que vaut notre modèle sur une base de donnée alternative?\n",
    "- De quelle manière pourroins nous améliorer les performances de notre modèle en l'entrainant sur d'autres bases de données ?\n",
    "\n",
    "### Bases de données accessibles avec PyTorch\n",
    "- MNIST, sur laquelle est entraîné le modèle\n",
    "- Fashion-MNIST, 28×28 - grayscale\t10 (vêtements)\tTest de généralisation sur données visuelles plus variées que MNIST \n",
    "- CIFAR-10\t32×32, RGB\t10\tImages naturelles, animaux et véhicules\n",
    "- CIFAR-100\t32×32, RGB\t100\tClassification plus fine, test de capacité à gérer plus de classes\n",
    "- SVHN (Street View House Numbers)\t32×32, RGB\t10\tReconnaissance de chiffres dans des contextes réels\n",
    "- MNIST-M\t28×28, RGB\t10\tMNIST modifié avec bruit / background → test de robustesse\n",
    "\n",
    "#### Mention spéciale pour les datasets Kaggle\n",
    "- Custom datasets\tVariable\tVariable\tImages médicales, industrielles, etc.\n",
    "\n",
    "### Critère de choix\n",
    "- **MNIST / Fashion-MNIST**\n",
    "    - Simple, rapide, parfait pour tester un modèle de base\n",
    "- **CIFAR / SVHN**\n",
    "    - Images colorées plus complexes → test de capacité à extraire des features plus fines\n",
    "- **MNIST-M / datasets bruités** \n",
    "    - Tester la robustesse aux perturbations et aux adversarial examples\n",
    "- **Datasets custom**\n",
    "    - Tester la fiabilité dans un contexte réel ou industriel\n",
    "\n",
    "### Adaptation du modèle en fonction des **objectifs** :\n",
    "- Généralisation vs robustesse\n",
    "- Tâche simple vs tâche complexe\n",
    "- Grayscale vs RGB\n",
    "\n",
    "### Quel serait le protocole le plus pertinent pour étudier les limites de notre modèle et améliorer ses performances en l'entrainant sur différentes bases de données?\n",
    "\n",
    "#### Définir les objectifs\n",
    "\n",
    "Avant tout, ce que tu veux mesurer :\n",
    "\n",
    "- Robustesse : comment le modèle résiste aux attaques adversariales ou aux perturbations.\n",
    "- Généralisation : performance sur des données nouvelles / différentes de l’entraînement.\n",
    "- Fiabilité : stabilité des prédictions en présence de bruit, transformations ou variations dans les données.\n",
    "- Performance pure : précision (accuracy), F1-score, etc. sur différentes bases\n",
    "\n",
    "#### Choisir les datasets\n",
    "\n",
    "Sélectionner plusieurs bases de données, avec des caractéristiques variées :\n",
    "\n",
    "- Simples et proches de MNIST : MNIST, Fashion-MNIST → test de base et débogage rapide.\n",
    "- Images colorées / plus complexes : CIFAR-10/100, SVHN → test de généralisation et capacité du modèle à extraire des features complexes.\n",
    "- Variantes bruitées / perturbées : MNIST-M, datasets augmentés → tester robustesse.\n",
    "- Custom / réelles : images médicales, industrielles → tester fiabilité et applicabilité réelle.\n",
    "\n",
    "#### Adapter le modèle\n",
    "\n",
    "- Ajuster entrée et sortie : nombre de canaux (grayscale → RGB), taille d’image, nombre de classes.\n",
    "\n",
    "- Éventuellement modifier l’architecture si le dataset est plus complexe : plus de filtres, couches, dropout, batch normalization.\n",
    "\n",
    "#### Définir les protocoles d’entraînement\n",
    "\n",
    "Pour chaque dataset :\n",
    "1. Mode standard (clean)\n",
    "    - Entraîner le modèle sur données clean.\n",
    "    - Tester sur données clean et adversariales.\n",
    "2. Mode adversarial training\n",
    "    - Générer adversarial examples (FGSM, PGD, etc.)\n",
    "    - Combiner loss clean et loss adversarial (alpha * clean + (1-alpha) * adv)\n",
    "    - Tester sur clean, adversarial, et éventuellement sur bruit/noise.\n",
    "3. Data augmentation / bruit\n",
    "    - Rotation, translation, scaling, bruit gaussien\n",
    "    - Vérifier si la performance et la robustesse s’améliorent.\n",
    "\n",
    "#### Faire varier les bons paramètres\n",
    "Pour chaque dataset, expérimenter avec :\n",
    "\n",
    "|Paramètre|\tObjectif|\n",
    "|-------------|--------------------------------|\n",
    "|Learning rate|Tester stabilité et vitesse d’apprentissage|\n",
    "|Batch size\t|Impact sur convergence et régularisation|\n",
    "|Epsilon (FGSM/PGD)\t|Mesurer robustesse face à perturbations|\n",
    "|Alpha adversarial\t|Trade-off clean vs robust accuracy|\n",
    "|Architecture\t|Nombre de filtres, couches → capacité du modèle|\n",
    "|Nombre d’époques\t|Sur- ou sous-entraînement|\n",
    "|Augmentation / bruit\t|Généralisation et fiabilité|\n",
    "\n",
    "#### Évaluation systématique\n",
    "\n",
    "Pour chaque expérience, mesurer :\n",
    "- Accuracy / Loss sur clean et adversarial\n",
    "- Courbes d’apprentissage (train/test loss vs epochs)\n",
    "- Comparaison graphique des modèles (clean vs adversarial)\n",
    "- Visualisation des adversarial examples et des perturbations\n",
    "- Analyse des erreurs : quelles classes sont les plus fragiles ?\n",
    "\n",
    "#### Comparaison inter-datasets\n",
    "\n",
    "- Observer comment le modèle réagit à différents types de données.\n",
    "- Identifier les datasets ou les conditions qui réduisent les performances.\n",
    "- Tester transfert de connaissances : entraîner sur dataset A, tester sur dataset B → mesure de la généralisation.\n",
    "\n",
    "#### Amélioration itérative\n",
    "\n",
    "À partir des observations :\n",
    "- Ajouter des couches, dropout, batch normalization pour plus de robustesse.\n",
    "- Ajuster alpha, epsilon pour adversarial training optimal.\n",
    "- Ajouter augmentation ou régularisation pour mieux généraliser.\n",
    "- Répéter le protocole sur plusieurs datasets pour confirmer la robustesse et fiabilité."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5375489",
   "metadata": {},
   "source": [
    "### Adaptation du code MNIST à un entrainement multidatasets\n",
    "\n",
    "- Adaptation de fgsm_attack : \n",
    "    - Détacher le tenseur et en créer un nouveau avec grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d344c5a",
   "metadata": {},
   "source": [
    "### Fashion-MNIST\n",
    "##### Entrainement de 2 modèles simples\n",
    "- Modèle normal, entrainé sur Fashion-MNIST clean\n",
    "- Modèle *adversarial*, adv. training --> génère adv. examples, mix clean+adv, modèle robuste\n",
    "\n",
    "![](clean_adv_diff-Fashion-MNIST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dfef8a",
   "metadata": {},
   "source": [
    "#### Analyse des résultats\n",
    "- \"Base\" model\n",
    "    - Clean accuracy = 88%\n",
    "    - Adv accuracy = 0.09-0.26% (perf détruites par FGSM)\n",
    "- \"adv\" model\n",
    "    - Clean accuracy = 85-86% (légère baisse)\n",
    "    - Adv accuracy = 78-85% (robuste ++)\n",
    "![](comparaison_performances-Fashion-MNIST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfbe477",
   "metadata": {},
   "source": [
    "### CIFAR-10\n",
    "##### Entrainement de 2 modèles simples\n",
    "- Modèle normal, entrainé sur CIFAR-10 clean\n",
    "- Modèle *adversarial*, adv. training --> génère adv. examples, mix clean+adv, modèle robuste\n",
    "\n",
    "![](clean_adv_diff_CIFAR-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35020ff4",
   "metadata": {},
   "source": [
    "#### Analyse des résultats\n",
    "- \"Base\" model\n",
    "    - Clean accuracy = 56-58%\n",
    "    - Adv accuracy = 0.23-44% (perf détruites par FGSM)\n",
    "- \"adv\" model\n",
    "    - Clean accuracy = 45% (légère baisse)\n",
    "    - Adv accuracy = 54% (robuste ++)\n",
    "![](comparaison_performances_CIFAR-10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12114f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eaf36d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "impact-epsilon",
   "metadata": {},
   "source": [
    "## Impact du paramètre \u03b5 (epsilon) sur la robustesse du mod\u00e8le\n",
    "\n",
    "### R\u00f4le du param\u00e8tre \u03b5\n",
    "\n",
    "Le param\u00e8tre \u03b5 (epsilon) contr\u00f4le l\u2019amplitude maximale de la perturbation appliqu\u00e9e aux images lors de la g\u00e9n\u00e9ration d\u2019exemples adversariaux avec l\u2019attaque **FGSM (Fast Gradient Sign Method)**.\n",
    "\n",
    "Dans FGSM, une perturbation est ajout\u00e9e \u00e0 l\u2019image d\u2019entr\u00e9e dans la direction du gradient de la fonction de perte par rapport aux pixels :\n",
    "\n",
    "$$\n",
    "x_{adv} = x + \\varepsilon \\cdot \\text{sign}(\\nabla_x L(x, y))\n",
    "$$\n",
    "\n",
    "Un \u03b5 faible correspond \u00e0 une perturbation quasi imperceptible \u00e0 l\u2019\u0153il humain, tandis qu\u2019un \u03b5 plus \u00e9lev\u00e9 peut fortement alt\u00e9rer l\u2019image mais augmente l\u2019efficacit\u00e9 de l\u2019attaque adversariale.\n",
    "\n",
    "---\n",
    "\n",
    "### Influence de \u03b5 sur les mod\u00e8les entra\u00een\u00e9s classiquement\n",
    "\n",
    "Les mod\u00e8les entra\u00een\u00e9s uniquement sur des donn\u00e9es *clean* se r\u00e9v\u00e8lent extr\u00eamement sensibles \u00e0 la valeur de \u03b5.\n",
    "\n",
    "- Sur **MNIST** et **Fashion-MNIST**, une augmentation mod\u00e9r\u00e9e de \u03b5 suffit \u00e0 faire chuter drastiquement la pr\u00e9cision adversariale, passant de valeurs \u00e9lev\u00e9es \u00e0 des performances proches de z\u00e9ro.\n",
    "- Sur **CIFAR-10**, cette sensibilit\u00e9 est encore plus marqu\u00e9e en raison de la complexit\u00e9 visuelle des images (textures, couleurs, d\u00e9tails fins).\n",
    "\n",
    "Cette vuln\u00e9rabilit\u00e9 s\u2019explique par le fait que les mod\u00e8les standards apprennent des fronti\u00e8res de d\u00e9cision tr\u00e8s locales, fortement d\u00e9pendantes de variations au niveau des pixels, ce qui les rend particuli\u00e8rement sensibles \u00e0 de petites perturbations dirig\u00e9es.\n",
    "\n",
    "---\n",
    "\n",
    "### Impact de \u03b5 dans le cadre de l\u2019entra\u00eenement adversarial\n",
    "\n",
    "Dans l\u2019entra\u00eenement adversarial, \u03b5 joue un r\u00f4le central dans le compromis entre **robustesse aux attaques** et **performance sur donn\u00e9es propres (clean accuracy)**.\n",
    "\n",
    "- **\u03b5 trop faible** : les perturbations g\u00e9n\u00e9r\u00e9es sont peu informatives, ce qui limite le gain en robustesse.\n",
    "- **\u03b5 mod\u00e9r\u00e9** : le mod\u00e8le apprend des repr\u00e9sentations plus stables, avec une am\u00e9lioration notable de la pr\u00e9cision adversariale et une baisse mod\u00e9r\u00e9e de la pr\u00e9cision clean.\n",
    "- **\u03b5 trop \u00e9lev\u00e9** : les exemples adversariaux deviennent trop \u00e9loign\u00e9s des donn\u00e9es originales, ce qui d\u00e9grade la performance sur donn\u00e9es propres et la g\u00e9n\u00e9ralisation.\n",
    "\n",
    "Dans nos exp\u00e9riences, les valeurs choisies (\u03b5 = 0.1 pour **Fashion-MNIST** et \u03b5 = 0.03 pour **CIFAR-10**) correspondent \u00e0 un compromis empirique permettant d\u2019am\u00e9liorer significativement la robustesse sans perte excessive de pr\u00e9cision sur donn\u00e9es propres.\n",
    "\n",
    "---\n",
    "\n",
    "### D\u00e9pendance de \u03b5 au dataset\n",
    "\n",
    "- **Datasets simples (MNIST, Fashion-MNIST)** : les images en niveaux de gris tol\u00e8rent des valeurs de \u03b5 relativement plus \u00e9lev\u00e9es sans alt\u00e9rer la s\u00e9mantique.\n",
    "- **Datasets complexes (CIFAR-10)** : les images RGB sont plus sensibles aux perturbations, n\u00e9cessitant des valeurs de \u03b5 plus faibles.\n",
    "\n",
    "Cela montre que \u03b5 ne peut pas \u00eatre choisi de mani\u00e8re universelle et doit \u00eatre adapt\u00e9 \u00e0 la complexit\u00e9 visuelle et aux caract\u00e9ristiques du dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### Limites et perspectives\n",
    "\n",
    "La robustesse acquise pour une valeur donn\u00e9e de \u03b5 reste sp\u00e9cifique \u00e0 cette valeur. Un mod\u00e8le entra\u00een\u00e9 avec un \u03b5 fixe peut rester vuln\u00e9rable \u00e0 des attaques utilisant d\u2019autres amplitudes ou des m\u00e9thodes plus puissantes comme **PGD**.\n",
    "\n",
    "Des pistes d\u2019am\u00e9lioration incluent l\u2019utilisation d\u2019un \u03b5 variable durant l\u2019entra\u00eenement, l\u2019\u00e9valuation sur une plage de valeurs de \u03b5 et la combinaison avec des attaques multi-\u00e9tapes.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Le param\u00e8tre \u03b5 constitue un hyperparam\u00e8tre cl\u00e9 de l\u2019entra\u00eenement adversarial. Il d\u00e9termine l\u2019intensit\u00e9 des perturbations, influence directement la robustesse du mod\u00e8le et conditionne le compromis fondamental entre pr\u00e9cision sur donn\u00e9es propres et r\u00e9sistance aux attaques adversariales.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
