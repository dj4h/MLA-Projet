{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efec2cdd",
   "metadata": {},
   "source": [
    "# Projet MLA\n",
    "## EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96559494",
   "metadata": {},
   "source": [
    "### MNIST\n",
    "\n",
    "#### Entrainement de 2 mod√®les simples\n",
    "- Mod√®le normal, entrain√© sur MNIST clean\n",
    "- Mod√®le *adversarial*, adv. training --> g√©n√®re adv. examples, mix clean+adv, mod√®le robuste\n",
    "\n",
    "![](clean_adv_diff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a5e7a5",
   "metadata": {},
   "source": [
    "#### Analyse des r√©sultats\n",
    "- \"Base\" model\n",
    "    - Clean accuracy = 98%\n",
    "    - Adv accuracy = 5-20% (perf d√©truites par FGSM)\n",
    "- \"adv\" model\n",
    "    - Clean accuracy = 96-98% (l√©g√®re baisse)\n",
    "    - Adv accuracy = 90-93% (robuste ++)\n",
    "![](comparaison_performances.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14d8e5",
   "metadata": {},
   "source": [
    "## Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec3011b",
   "metadata": {},
   "source": [
    "## Param√®tres √† faire varier pour √©tudier la robustesse et la fiabilit√©\n",
    "|**Param√®tre |Effet attendu  / Ce que √ßa teste** |\n",
    "|----------|----------------------------------|\n",
    "|**Epsilon (FGSM / adversarial attack)**|Intensit√© de perturbation ‚Üí robustesse du mod√®le aux attaques|\n",
    "|**Alpha dans adversarial training**|Poids entre loss clean et loss adversarial ‚Üí impact sur trade-off clean vs robust accuracy|\n",
    "|**Batch size**|Influence la convergence et stabilit√© de l‚Äôentra√Ænement|\n",
    "|**Learning rate**|Trop grand ‚Üí mod√®le instable, trop petit ‚Üí apprentissage lent|\n",
    "|**Nombre d‚Äô√©poques**|V√©rifier si le mod√®le sur-entra√Æne ou sous-entra√Æne|\n",
    "|**Architecture du mod√®le (nombres de filtres, couches, FC)**|\tTeste si plus complexe = meilleure robustesse ou sur-apprentissage\n",
    "|**Type d‚Äôattaque (FGSM, PGD, DeepFool, etc.)**|Teste la robustesse face √† diff√©rentes perturbations|\n",
    "|**Ajout de bruit al√©atoire / transformations**|Data augmentation pour tester g√©n√©ralisation et fiabilit√© sur images l√©g√®rement modifi√©es|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7878a2af",
   "metadata": {},
   "source": [
    "## √âvaluation du mod√®le sur d'autres base de donn√©es\n",
    "\n",
    "### Pourquoi?\n",
    "Dans la 1√®re partie on a pu r√©impl√©menter les √©tapes de constructions du mod√®le d√©crit dans l'article. √Ä partir de la base de donn√©e MNIST de PyTorch, le mod√®le a pu √™tre entra√Æn√© pour √©tablir la m√©thode d'entra√Ænement adversarial la plus adapt√©e au traitement de ces donn√©es. \n",
    "Toutefois, nous sommes en doit de nous demander :\n",
    "- Que vaut notre mod√®le sur une base de donn√©e alternative?\n",
    "- De quelle mani√®re pourroins nous am√©liorer les performances de notre mod√®le en l'entrainant sur d'autres bases de donn√©es ?\n",
    "\n",
    "### Bases de donn√©es accessibles avec PyTorch\n",
    "- MNIST, sur laquelle est entra√Æn√© le mod√®le\n",
    "- Fashion-MNIST, 28√ó28 - grayscale\t10 (v√™tements)\tTest de g√©n√©ralisation sur donn√©es visuelles plus vari√©es que MNIST \n",
    "- CIFAR-10\t32√ó32, RGB\t10\tImages naturelles, animaux et v√©hicules\n",
    "- CIFAR-100\t32√ó32, RGB\t100\tClassification plus fine, test de capacit√© √† g√©rer plus de classes\n",
    "- SVHN (Street View House Numbers)\t32√ó32, RGB\t10\tReconnaissance de chiffres dans des contextes r√©els\n",
    "- MNIST-M\t28√ó28, RGB\t10\tMNIST modifi√© avec bruit / background ‚Üí test de robustesse\n",
    "\n",
    "#### Mention sp√©ciale pour les datasets Kaggle\n",
    "- Custom datasets\tVariable\tVariable\tImages m√©dicales, industrielles, etc.\n",
    "\n",
    "### Crit√®re de choix\n",
    "- **MNIST / Fashion-MNIST**\n",
    "    - Simple, rapide, parfait pour tester un mod√®le de base\n",
    "- **CIFAR / SVHN**\n",
    "    - Images color√©es plus complexes ‚Üí test de capacit√© √† extraire des features plus fines\n",
    "- **MNIST-M / datasets bruit√©s** \n",
    "    - Tester la robustesse aux perturbations et aux adversarial examples\n",
    "- **Datasets custom**\n",
    "    - Tester la fiabilit√© dans un contexte r√©el ou industriel\n",
    "\n",
    "### Adaptation du mod√®le en fonction des **objectifs** :\n",
    "- G√©n√©ralisation vs robustesse\n",
    "- T√¢che simple vs t√¢che complexe\n",
    "- Grayscale vs RGB\n",
    "\n",
    "### Quel serait le protocole le plus pertinent pour √©tudier les limites de notre mod√®le et am√©liorer ses performances en l'entrainant sur diff√©rentes bases de donn√©es?\n",
    "\n",
    "#### D√©finir les objectifs\n",
    "\n",
    "Avant tout, ce que tu veux mesurer :\n",
    "\n",
    "- Robustesse : comment le mod√®le r√©siste aux attaques adversariales ou aux perturbations.\n",
    "- G√©n√©ralisation : performance sur des donn√©es nouvelles / diff√©rentes de l‚Äôentra√Ænement.\n",
    "- Fiabilit√© : stabilit√© des pr√©dictions en pr√©sence de bruit, transformations ou variations dans les donn√©es.\n",
    "- Performance pure : pr√©cision (accuracy), F1-score, etc. sur diff√©rentes bases\n",
    "\n",
    "#### Choisir les datasets\n",
    "\n",
    "S√©lectionner plusieurs bases de donn√©es, avec des caract√©ristiques vari√©es :\n",
    "\n",
    "- Simples et proches de MNIST : MNIST, Fashion-MNIST ‚Üí test de base et d√©bogage rapide.\n",
    "- Images color√©es / plus complexes : CIFAR-10/100, SVHN ‚Üí test de g√©n√©ralisation et capacit√© du mod√®le √† extraire des features complexes.\n",
    "- Variantes bruit√©es / perturb√©es : MNIST-M, datasets augment√©s ‚Üí tester robustesse.\n",
    "- Custom / r√©elles : images m√©dicales, industrielles ‚Üí tester fiabilit√© et applicabilit√© r√©elle.\n",
    "\n",
    "#### Adapter le mod√®le\n",
    "\n",
    "- Ajuster entr√©e et sortie : nombre de canaux (grayscale ‚Üí RGB), taille d‚Äôimage, nombre de classes.\n",
    "\n",
    "- √âventuellement modifier l‚Äôarchitecture si le dataset est plus complexe : plus de filtres, couches, dropout, batch normalization.\n",
    "\n",
    "#### D√©finir les protocoles d‚Äôentra√Ænement\n",
    "\n",
    "Pour chaque dataset :\n",
    "1. Mode standard (clean)\n",
    "    - Entra√Æner le mod√®le sur donn√©es clean.\n",
    "    - Tester sur donn√©es clean et adversariales.\n",
    "2. Mode adversarial training\n",
    "    - G√©n√©rer adversarial examples (FGSM, PGD, etc.)\n",
    "    - Combiner loss clean et loss adversarial (alpha * clean + (1-alpha) * adv)\n",
    "    - Tester sur clean, adversarial, et √©ventuellement sur bruit/noise.\n",
    "3. Data augmentation / bruit\n",
    "    - Rotation, translation, scaling, bruit gaussien\n",
    "    - V√©rifier si la performance et la robustesse s‚Äôam√©liorent.\n",
    "\n",
    "#### Faire varier les bons param√®tres\n",
    "Pour chaque dataset, exp√©rimenter avec :\n",
    "\n",
    "|Param√®tre|\tObjectif|\n",
    "|-------------|--------------------------------|\n",
    "|Learning rate|Tester stabilit√© et vitesse d‚Äôapprentissage|\n",
    "|Batch size\t|Impact sur convergence et r√©gularisation|\n",
    "|Epsilon (FGSM/PGD)\t|Mesurer robustesse face √† perturbations|\n",
    "|Alpha adversarial\t|Trade-off clean vs robust accuracy|\n",
    "|Architecture\t|Nombre de filtres, couches ‚Üí capacit√© du mod√®le|\n",
    "|Nombre d‚Äô√©poques\t|Sur- ou sous-entra√Ænement|\n",
    "|Augmentation / bruit\t|G√©n√©ralisation et fiabilit√©|\n",
    "\n",
    "#### √âvaluation syst√©matique\n",
    "\n",
    "Pour chaque exp√©rience, mesurer :\n",
    "- Accuracy / Loss sur clean et adversarial\n",
    "- Courbes d‚Äôapprentissage (train/test loss vs epochs)\n",
    "- Comparaison graphique des mod√®les (clean vs adversarial)\n",
    "- Visualisation des adversarial examples et des perturbations\n",
    "- Analyse des erreurs : quelles classes sont les plus fragiles ?\n",
    "\n",
    "#### Comparaison inter-datasets\n",
    "\n",
    "- Observer comment le mod√®le r√©agit √† diff√©rents types de donn√©es.\n",
    "- Identifier les datasets ou les conditions qui r√©duisent les performances.\n",
    "- Tester transfert de connaissances : entra√Æner sur dataset A, tester sur dataset B ‚Üí mesure de la g√©n√©ralisation.\n",
    "\n",
    "#### Am√©lioration it√©rative\n",
    "\n",
    "√Ä partir des observations :\n",
    "- Ajouter des couches, dropout, batch normalization pour plus de robustesse.\n",
    "- Ajuster alpha, epsilon pour adversarial training optimal.\n",
    "- Ajouter augmentation ou r√©gularisation pour mieux g√©n√©raliser.\n",
    "- R√©p√©ter le protocole sur plusieurs datasets pour confirmer la robustesse et fiabilit√©."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5375489",
   "metadata": {},
   "source": [
    "### Adaptation du code MNIST √† un entrainement multidatasets\n",
    "\n",
    "- Adaptation de fgsm_attack : \n",
    "    - D√©tacher le tenseur et en cr√©er un nouveau avec grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d344c5a",
   "metadata": {},
   "source": [
    "### Fashion-MNIST\n",
    "##### Entrainement de 2 mod√®les simples\n",
    "- Mod√®le normal, entrain√© sur Fashion-MNIST clean\n",
    "- Mod√®le *adversarial*, adv. training --> g√©n√®re adv. examples, mix clean+adv, mod√®le robuste\n",
    "\n",
    "![](clean_adv_diff-Fashion-MNIST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dfef8a",
   "metadata": {},
   "source": [
    "#### Analyse des r√©sultats\n",
    "- \"Base\" model\n",
    "    - Clean accuracy = 88%\n",
    "    - Adv accuracy = 0.09-0.26% (perf d√©truites par FGSM)\n",
    "- \"adv\" model\n",
    "    - Clean accuracy = 85-86% (l√©g√®re baisse)\n",
    "    - Adv accuracy = 78-85% (robuste ++)\n",
    "![](comparaison_performances-Fashion-MNIST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfbe477",
   "metadata": {},
   "source": [
    "### CIFAR-10\n",
    "##### Entrainement de 2 mod√®les simples\n",
    "- Mod√®le normal, entrain√© sur CIFAR-10 clean\n",
    "- Mod√®le *adversarial*, adv. training --> g√©n√®re adv. examples, mix clean+adv, mod√®le robuste\n",
    "\n",
    "![](clean_adv_diff_CIFAR-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35020ff4",
   "metadata": {},
   "source": [
    "#### Analyse des r√©sultats\n",
    "- \"Base\" model\n",
    "    - Clean accuracy = 56-58%\n",
    "    - Adv accuracy = 0.23-44% (perf d√©truites par FGSM)\n",
    "- \"adv\" model\n",
    "    - Clean accuracy = 45% (l√©g√®re baisse)\n",
    "    - Adv accuracy = 54% (robuste ++)\n",
    "![](comparaison_performances_CIFAR-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c04e177",
   "metadata": {},
   "source": [
    "![](clean_adv_diff.png)\n",
    "\n",
    "![](pgd_attack_MNIST.png)\n",
    "\n",
    "Code couleur pour la diff√©rence, directions d‚Äôam√©lioration donn√©es par le gradient\n",
    "- rouge : gradient positif (augmenter le pixel)\n",
    "- bleu : gradient n√©gatif (diminuer le pixel)\n",
    "- blanc : gradient nul\n",
    "\n",
    "La visualisation du gradient met en √©vidence les r√©gions de l‚Äôimage les plus sensibles pour le mod√®le ainsi que les directions dans lesquelles les pixels doivent √™tre modifi√©s pour augmenter la perte.\n",
    "Les zones rouges et bleues, concentr√©es sur les traits du chiffre, montrent que le r√©seau repose sur des motifs locaux pr√©cis, tandis que les zones blanches indiquent des pixels peu informatifs.\n",
    "Ces cartes de gradient fournissent une interpr√©tation directe du fonctionnement des attaques adversariales comme FGSM et PGD.\n",
    "\n",
    "FGSM applique une perturbation uniforme √† partir du signe du gradient alors que PGD affiine la perturbation √©tape par √©tape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbcdaca",
   "metadata": {},
   "source": [
    "# Lien avec le cours\n",
    "On travail avec CNN => class SimpleCNN(nn.Module)\n",
    "- exploite la localit√© (bords, formes)\n",
    "- partage des poids\n",
    "- champ r√©ceptif progressif\n",
    "\n",
    "### Notre mod√®le est une version simplifi√©e de LeNet-5\n",
    "        Convolution\tnn.Conv2d\n",
    "    Activation tanh\tReLU\n",
    "    Average pooling\tMaxPool2d\n",
    "    Fully connected\tnn.Linear\n",
    "              MNIST\tMNIST\n",
    "mm principe architectural\n",
    "\n",
    "### Padding pour pr√©server l‚Äôinformation des bords\n",
    "- Padding = 1 ‚Üí \"same convolution\"\n",
    "- La taille reste 28√ó28\n",
    "\n",
    "### Augmentation du champ r√©ceptif effectif\n",
    "- Pooling = stride indirect\n",
    "- Fen√™tre 2√ó2\n",
    "- Stride = 2\n",
    "- Taille divis√©e par 2 ‚Üí 14√ó14\n",
    "Un neurone profond ne regarde plus un pixel, mais une zone √©tendue de l‚Äôimage.\n",
    "\n",
    "### MLP en fin de r√©seau\n",
    "\"self.fc1 = nn.Linear(64 * 14 * 14, 128)\"\n",
    "\"self.fc2 = nn.Linear(128, 10)\"\n",
    "- Le CNN extrait des features visuelles\n",
    "- Le MLP :\n",
    "    - combine ces features\n",
    "    - fait la d√©cision finale\n",
    "\n",
    "CNN = extracteur\n",
    "MLP = classifieur\n",
    "\n",
    "### Pourquoi le CNN est vuln√©rable ?\n",
    "\n",
    "Parce que :\n",
    "- il agr√®ge beaucoup d‚Äôinformations locales\n",
    "- une petite perturbation pixel par pixel\n",
    "- se propage dans tout le r√©seau\n",
    "PGD est plus fort que FGSM car :\n",
    "- il exploite progressivement le gradient\n",
    "- il reste dans la boule Œµ\n",
    "- il cible les zones sensibles du champ r√©ceptif\n",
    "\n",
    "### Pourquoi l‚Äôadversarial training am√©liore la robustesse ?\n",
    "loss = alpha * loss_clean + (1 - alpha) * loss_adv\n",
    "\n",
    "On force le CNN √† :\n",
    "- apprendre des features plus stables\n",
    "- r√©duire la sensibilit√© aux micro-variations\n",
    "- lisser la fonction de d√©cision\n",
    "Conceptuellement, on modifie la g√©om√©trie de l‚Äôespace des features\n",
    "\n",
    "### R√©sum√© conceptuel \n",
    "       Concept th√©orique  O√π il appara√Æt dans ton code\n",
    "                    CNN\tConv2d, MaxPool2d\n",
    "                LeNet-5\tArchitecture globale\n",
    "                    MLP\tLinear\n",
    "         Champ r√©ceptif\tEmpilement conv + pooling\n",
    "    Stride / Padding       Param√®tres des conv / pool\n",
    "        Backpropagation\tloss.backward()\n",
    "               Gradient    FGSM / PGD\n",
    "            Robustesse     Adversarial training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a92065f",
   "metadata": {},
   "source": [
    "## Am√©lioration du code et lien avec le cours\n",
    "\n",
    "### Auto-encodeurs : d√©fense et interpr√©tation adversariale\n",
    "#### Id√©e cl√©\n",
    "Utiliser un auto-encodeur comme filtre de reconstruction avant le classifieur.\n",
    "\n",
    "#### Hypoth√®se :\n",
    "Les perturbations adversariales sont souvent haute fr√©quence et non naturelles.\n",
    "Un auto-encodeur entra√Æn√© sur des images propres peut les att√©nuer.\n",
    "\n",
    "- pas besoin de changer le CNN, seulement de pr√©traiter l‚Äôentr√©e\n",
    "\n",
    "#### √âtude\n",
    "- Accuracy clean vs FGSM vs PGD avec et sans auto-encodeur\n",
    "- Impact de Œµ avant / apr√®s reconstruction\n",
    "- Visualisation :\n",
    "    - image originale\n",
    "    - image adversariale\n",
    "    - image reconstruite\n",
    "    - diff√©rence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73dae45b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1280/662932144.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mAutoEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         self.encoder = nn.Sequential(\n\u001b[0;32m      5\u001b[0m             \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# 16x14x14\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# entra√Æner uniquement sur images clean\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),  # 16x14x14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1), # 32x7x7\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011fffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √©valuation, dans test FGSM / PGD\n",
    "with torch.no_grad():\n",
    "    x_recon = autoencoder(x_adv)\n",
    "    output = model(x_recon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bc8ad2",
   "metadata": {},
   "source": [
    "#### MNIST / Fashion-MNIST\n",
    "- Tr√®s pertinent\n",
    "- Reconstruction facile\n",
    "- Perturbations adversariales bien visibles\n",
    "L‚Äôauto-encodeur agit comme un filtre basse fr√©quence efficace\n",
    "\n",
    "Conclusion : am√©lioration claire de la robustesse FGSM, parfois PGD l√©ger.\n",
    "\n",
    "#### CIFAR-10\n",
    "- pruning mod√©r√© utile\n",
    "- trop de pruning ‚Üí chute clean + robuste\n",
    "int√©ressant pour montrer un compromis capacit√© / robustesse\n",
    "\n",
    "Les mod√®les surparam√©tr√©s sont souvent plus vuln√©rables aux attaques locales.\n",
    "\n",
    "    D√©fense\tMNIST\tCIFAR-10\n",
    "    FGSM training\t‚úî\t‚úî\n",
    "    AE preprocessing\t‚úî‚úî\t‚ö†Ô∏è\n",
    "    Pruning\t‚úî\t‚úî\n",
    "    PGD robustness\t‚ùå\t‚ùå\n",
    "Aucune d√©fense simple ne g√©n√©ralise parfaitement. La robustesse d√©pend √† la fois du mod√®le, de la d√©fense et de la structure du dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ba9fa1",
   "metadata": {},
   "source": [
    "### Auto-encodeur = compression de l‚Äôinformation\n",
    "\n",
    "Tu peux aussi pr√©senter l‚ÄôAE comme une compression non lin√©aire de l‚Äôimage\n",
    "\n",
    "üîç Lien direct avec la robustesse\n",
    "- Compression ‚áí perte d‚Äôinformation fine\n",
    "- Les perturbations adversariales sont souvent tr√®s fines\n",
    "- Donc elles sont √©cras√©es par la compression\n",
    "\n",
    "üëâ Tu peux faire varier :\n",
    "- taille du latent space\n",
    "- taux de compression\n",
    "Et mesurer :\n",
    "- accuracy clean\n",
    "- accuracy FGSM / PGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a6db3d",
   "metadata": {},
   "source": [
    "### Compression du r√©seau (pruning & quantization)\n",
    "üí° Pourquoi c‚Äôest pertinent ici ?\n",
    "Il existe un lien fort entre :\n",
    "- complexit√© du mod√®le\n",
    "- sensibilit√© aux perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0dfd635",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1280/1935181862.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Pruning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprune\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mprune\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m prune.l1_unstructured(\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Pruning\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "prune.l1_unstructured(\n",
    "    model.fc1,\n",
    "    name=\"weight\",\n",
    "    amount=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fc0a7e",
   "metadata": {},
   "source": [
    "comparer :\n",
    "- mod√®le dense\n",
    "- mod√®le prun√©\n",
    "- robustesse FGSM / PGD\n",
    "\n",
    "#### Hypoth√®se int√©ressante\n",
    "\n",
    "Un mod√®le plus simple peut parfois √™tre moins sensible aux perturbations locales\n",
    "\n",
    "(ce n‚Äôest pas toujours vrai ‚Üí tr√®s bon point de discussion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d70237e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "impact-epsilon",
   "metadata": {},
   "source": [
    "## Impact du param√®tre \u03b5 (epsilon) sur la robustesse du mod\u00e8le\n",
    "\n",
    "### R\u00f4le du param\u00e8tre \u03b5\n",
    "\n",
    "Le param\u00e8tre \u03b5 (epsilon) contr\u00f4le l\u2019amplitude maximale de la perturbation appliqu\u00e9e aux images lors de la g\u00e9n\u00e9ration d\u2019exemples adversariaux avec l\u2019attaque **FGSM (Fast Gradient Sign Method)**.\n",
    "\n",
    "Dans FGSM, une perturbation est ajout\u00e9e \u00e0 l\u2019image d\u2019entr\u00e9e dans la direction du gradient de la fonction de perte par rapport aux pixels :\n",
    "\n",
    "$$\n",
    "x_{adv} = x + \\varepsilon \\cdot \\text{sign}(\\nabla_x L(x, y))\n",
    "$$\n",
    "\n",
    "Un \u03b5 faible correspond \u00e0 une perturbation quasi imperceptible \u00e0 l\u2019\u0153il humain, tandis qu\u2019un \u03b5 plus \u00e9lev\u00e9 peut fortement alt\u00e9rer l\u2019image mais augmente l\u2019efficacit\u00e9 de l\u2019attaque adversariale.\n",
    "\n",
    "---\n",
    "\n",
    "### Influence de \u03b5 sur les mod\u00e8les entra\u00een\u00e9s classiquement\n",
    "\n",
    "Les mod\u00e8les entra\u00een\u00e9s uniquement sur des donn\u00e9es *clean* se r\u00e9v\u00e8lent extr\u00eamement sensibles \u00e0 la valeur de \u03b5.\n",
    "\n",
    "- Sur **MNIST** et **Fashion-MNIST**, une augmentation mod\u00e9r\u00e9e de \u03b5 suffit \u00e0 faire chuter drastiquement la pr\u00e9cision adversariale, passant de valeurs \u00e9lev\u00e9es \u00e0 des performances proches de z\u00e9ro.\n",
    "- Sur **CIFAR-10**, cette sensibilit\u00e9 est encore plus marqu\u00e9e en raison de la complexit\u00e9 visuelle des images (textures, couleurs, d\u00e9tails fins).\n",
    "\n",
    "Cette vuln\u00e9rabilit\u00e9 s\u2019explique par le fait que les mod\u00e8les standards apprennent des fronti\u00e8res de d\u00e9cision tr\u00e8s locales, fortement d\u00e9pendantes de variations au niveau des pixels, ce qui les rend particuli\u00e8rement sensibles \u00e0 de petites perturbations dirig\u00e9es.\n",
    "\n",
    "---\n",
    "\n",
    "### Impact de \u03b5 dans le cadre de l\u2019entra\u00eenement adversarial\n",
    "\n",
    "Dans l\u2019entra\u00eenement adversarial, \u03b5 joue un r\u00f4le central dans le compromis entre **robustesse aux attaques** et **performance sur donn\u00e9es propres (clean accuracy)**.\n",
    "\n",
    "- **\u03b5 trop faible** : les perturbations g\u00e9n\u00e9r\u00e9es sont peu informatives, ce qui limite le gain en robustesse.\n",
    "- **\u03b5 mod\u00e9r\u00e9** : le mod\u00e8le apprend des repr\u00e9sentations plus stables, avec une am\u00e9lioration notable de la pr\u00e9cision adversariale et une baisse mod\u00e9r\u00e9e de la pr\u00e9cision clean.\n",
    "- **\u03b5 trop \u00e9lev\u00e9** : les exemples adversariaux deviennent trop \u00e9loign\u00e9s des donn\u00e9es originales, ce qui d\u00e9grade la performance sur donn\u00e9es propres et la g\u00e9n\u00e9ralisation.\n",
    "\n",
    "Dans nos exp\u00e9riences, les valeurs choisies (\u03b5 = 0.1 pour **Fashion-MNIST** et \u03b5 = 0.03 pour **CIFAR-10**) correspondent \u00e0 un compromis empirique permettant d\u2019am\u00e9liorer significativement la robustesse sans perte excessive de pr\u00e9cision sur donn\u00e9es propres.\n",
    "\n",
    "---\n",
    "\n",
    "### D\u00e9pendance de \u03b5 au dataset\n",
    "\n",
    "- **Datasets simples (MNIST, Fashion-MNIST)** : les images en niveaux de gris tol\u00e8rent des valeurs de \u03b5 relativement plus \u00e9lev\u00e9es sans alt\u00e9rer la s\u00e9mantique.\n",
    "- **Datasets complexes (CIFAR-10)** : les images RGB sont plus sensibles aux perturbations, n\u00e9cessitant des valeurs de \u03b5 plus faibles.\n",
    "\n",
    "Cela montre que \u03b5 ne peut pas \u00eatre choisi de mani\u00e8re universelle et doit \u00eatre adapt\u00e9 \u00e0 la complexit\u00e9 visuelle et aux caract\u00e9ristiques du dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### Limites et perspectives\n",
    "\n",
    "La robustesse acquise pour une valeur donn\u00e9e de \u03b5 reste sp\u00e9cifique \u00e0 cette valeur. Un mod\u00e8le entra\u00een\u00e9 avec un \u03b5 fixe peut rester vuln\u00e9rable \u00e0 des attaques utilisant d\u2019autres amplitudes ou des m\u00e9thodes plus puissantes comme **PGD**.\n",
    "\n",
    "Des pistes d\u2019am\u00e9lioration incluent l\u2019utilisation d\u2019un \u03b5 variable durant l\u2019entra\u00eenement, l\u2019\u00e9valuation sur une plage de valeurs de \u03b5 et la combinaison avec des attaques multi-\u00e9tapes.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Le param\u00e8tre \u03b5 constitue un hyperparam\u00e8tre cl\u00e9 de l\u2019entra\u00eenement adversarial. Il d\u00e9termine l\u2019intensit\u00e9 des perturbations, influence directement la robustesse du mod\u00e8le et conditionne le compromis fondamental entre pr\u00e9cision sur donn\u00e9es propres et r\u00e9sistance aux attaques adversariales.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
},
