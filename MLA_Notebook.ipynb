{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efec2cdd",
   "metadata": {},
   "source": [
    "# Projet MLA\n",
    "## EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96559494",
   "metadata": {},
   "source": [
    "### MNIST\n",
    "\n",
    "#### Entrainement de 2 modèles simples\n",
    "- Modèle normal, entrainé sur MNIST clean\n",
    "- Modèle *adversarial*, adv. training --> génère adv. examples, mix clean+adv, modèle robuste\n",
    "\n",
    "![](clean_adv_diff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a5e7a5",
   "metadata": {},
   "source": [
    "#### Analyse des résultats\n",
    "- \"Base\" model\n",
    "    - Clean accuracy = 98%\n",
    "    - Adv accuracy = 5-20% (perf détruites par FGSM)\n",
    "- \"adv\" model\n",
    "    - Clean accuracy = 96-98% (légère baisse)\n",
    "    - Adv accuracy = 90-93% (robuste ++)\n",
    "![](comparaison_performances.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14d8e5",
   "metadata": {},
   "source": [
    "## Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec3011b",
   "metadata": {},
   "source": [
    "## Paramètres à faire varier pour étudier la robustesse et la fiabilité\n",
    "|**Paramètre |Effet attendu  / Ce que ça teste** |\n",
    "|----------|----------------------------------|\n",
    "|**Epsilon (FGSM / adversarial attack)**|Intensité de perturbation → robustesse du modèle aux attaques|\n",
    "|**Alpha dans adversarial training**|Poids entre loss clean et loss adversarial → impact sur trade-off clean vs robust accuracy|\n",
    "|**Batch size**|Influence la convergence et stabilité de l’entraînement|\n",
    "|**Learning rate**|Trop grand → modèle instable, trop petit → apprentissage lent|\n",
    "|**Nombre d’époques**|Vérifier si le modèle sur-entraîne ou sous-entraîne|\n",
    "|**Architecture du modèle (nombres de filtres, couches, FC)**|\tTeste si plus complexe = meilleure robustesse ou sur-apprentissage\n",
    "|**Type d’attaque (FGSM, PGD, DeepFool, etc.)**|Teste la robustesse face à différentes perturbations|\n",
    "|**Ajout de bruit aléatoire / transformations**|Data augmentation pour tester généralisation et fiabilité sur images légèrement modifiées|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7878a2af",
   "metadata": {},
   "source": [
    "## Évaluation du modèle sur d'autres base de données\n",
    "\n",
    "### Pourquoi?\n",
    "Dans la 1ère partie on a pu réimplémenter les étapes de constructions du modèle décrit dans l'article. À partir de la base de donnée MNIST de PyTorch, le modèle a pu être entraîné pour établir la méthode d'entraînement adversarial la plus adaptée au traitement de ces données. (/p)\n",
    "Toutefois, nous sommes en doit de nous demander :\n",
    "- Que vaut notre modèle sur une base de donnée alternative?\n",
    "- De quelle manière pourroins nous améliorer les performances de notre modèle en l'entrainant sur d'autres bases de données ?\n",
    "\n",
    "### Bases de données accessibles avec PyTorch\n",
    "- MNIST, sur laquelle est entraîné le modèle\n",
    "- Fashion-MNIST, 28×28 - grayscale\t10 (vêtements)\tTest de généralisation sur données visuelles plus variées que MNIST \n",
    "- CIFAR-10\t32×32, RGB\t10\tImages naturelles, animaux et véhicules\n",
    "- CIFAR-100\t32×32, RGB\t100\tClassification plus fine, test de capacité à gérer plus de classes\n",
    "- SVHN (Street View House Numbers)\t32×32, RGB\t10\tReconnaissance de chiffres dans des contextes réels\n",
    "- MNIST-M\t28×28, RGB\t10\tMNIST modifié avec bruit / background → test de robustesse\n",
    "\n",
    "#### Mention spéciale pour les datasets Kaggle\n",
    "- Custom datasets\tVariable\tVariable\tImages médicales, industrielles, etc.\n",
    "\n",
    "### Critère de choix\n",
    "- **MNIST / Fashion-MNIST**\n",
    "    - Simple, rapide, parfait pour tester un modèle de base\n",
    "- **CIFAR / SVHN**\n",
    "    - Images colorées plus complexes → test de capacité à extraire des features plus fines\n",
    "- **MNIST-M / datasets bruités** \n",
    "    - Tester la robustesse aux perturbations et aux adversarial examples\n",
    "- **Datasets custom**\n",
    "    - Tester la fiabilité dans un contexte réel ou industriel\n",
    "\n",
    "### Adaptation du modèle en fonction des **objectifs** :\n",
    "- Généralisation vs robustesse\n",
    "- Tâche simple vs tâche complexe\n",
    "- Grayscale vs RGB\n",
    "\n",
    "### Quel serait le protocole le plus pertinent pour étudier les limites de notre modèle et améliorer ses performances en l'entrainant sur différentes bases de données?\n",
    "\n",
    "#### Définir les objectifs\n",
    "\n",
    "Avant tout, ce que tu veux mesurer :\n",
    "\n",
    "- Robustesse : comment le modèle résiste aux attaques adversariales ou aux perturbations.\n",
    "- Généralisation : performance sur des données nouvelles / différentes de l’entraînement.\n",
    "- Fiabilité : stabilité des prédictions en présence de bruit, transformations ou variations dans les données.\n",
    "- Performance pure : précision (accuracy), F1-score, etc. sur différentes bases\n",
    "\n",
    "#### Choisir les datasets\n",
    "\n",
    "Sélectionner plusieurs bases de données, avec des caractéristiques variées :\n",
    "\n",
    "- Simples et proches de MNIST : MNIST, Fashion-MNIST → test de base et débogage rapide.\n",
    "- Images colorées / plus complexes : CIFAR-10/100, SVHN → test de généralisation et capacité du modèle à extraire des features complexes.\n",
    "- Variantes bruitées / perturbées : MNIST-M, datasets augmentés → tester robustesse.\n",
    "- Custom / réelles : images médicales, industrielles → tester fiabilité et applicabilité réelle.\n",
    "\n",
    "#### Adapter le modèle\n",
    "\n",
    "- Ajuster entrée et sortie : nombre de canaux (grayscale → RGB), taille d’image, nombre de classes.\n",
    "\n",
    "- Éventuellement modifier l’architecture si le dataset est plus complexe : plus de filtres, couches, dropout, batch normalization.\n",
    "\n",
    "#### Définir les protocoles d’entraînement\n",
    "\n",
    "Pour chaque dataset :\n",
    "1. Mode standard (clean)\n",
    "    - Entraîner le modèle sur données clean.\n",
    "    - Tester sur données clean et adversariales.\n",
    "2. Mode adversarial training\n",
    "    - Générer adversarial examples (FGSM, PGD, etc.)\n",
    "    - Combiner loss clean et loss adversarial (alpha * clean + (1-alpha) * adv)\n",
    "    - Tester sur clean, adversarial, et éventuellement sur bruit/noise.\n",
    "3. Data augmentation / bruit\n",
    "    - Rotation, translation, scaling, bruit gaussien\n",
    "    - Vérifier si la performance et la robustesse s’améliorent.\n",
    "\n",
    "#### Faire varier les bons paramètres\n",
    "Pour chaque dataset, expérimenter avec :\n",
    "\n",
    "|Paramètre|\tObjectif|\n",
    "|-------------|--------------------------------|\n",
    "|Learning rate|Tester stabilité et vitesse d’apprentissage|\n",
    "|Batch size\t|Impact sur convergence et régularisation|\n",
    "|Epsilon (FGSM/PGD)\t|Mesurer robustesse face à perturbations|\n",
    "|Alpha adversarial\t|Trade-off clean vs robust accuracy|\n",
    "|Architecture\t|Nombre de filtres, couches → capacité du modèle|\n",
    "|Nombre d’époques\t|Sur- ou sous-entraînement|\n",
    "|Augmentation / bruit\t|Généralisation et fiabilité|\n",
    "\n",
    "#### Évaluation systématique\n",
    "\n",
    "Pour chaque expérience, mesurer :\n",
    "- Accuracy / Loss sur clean et adversarial\n",
    "- Courbes d’apprentissage (train/test loss vs epochs)\n",
    "- Comparaison graphique des modèles (clean vs adversarial)\n",
    "- Visualisation des adversarial examples et des perturbations\n",
    "- Analyse des erreurs : quelles classes sont les plus fragiles ?\n",
    "\n",
    "#### Comparaison inter-datasets\n",
    "\n",
    "- Observer comment le modèle réagit à différents types de données.\n",
    "- Identifier les datasets ou les conditions qui réduisent les performances.\n",
    "- Tester transfert de connaissances : entraîner sur dataset A, tester sur dataset B → mesure de la généralisation.\n",
    "\n",
    "#### Amélioration itérative\n",
    "\n",
    "À partir des observations :\n",
    "- Ajouter des couches, dropout, batch normalization pour plus de robustesse.\n",
    "- Ajuster alpha, epsilon pour adversarial training optimal.\n",
    "- Ajouter augmentation ou régularisation pour mieux généraliser.\n",
    "- Répéter le protocole sur plusieurs datasets pour confirmer la robustesse et fiabilité."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5375489",
   "metadata": {},
   "source": [
    "### Adaptation du code MNIST à un entrainement multidatasets\n",
    "\n",
    "- Adaptation de fgsm_attack : \n",
    "    - Détacher le tenseur et en créer un nouveau avec grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d344c5a",
   "metadata": {},
   "source": [
    "### Fashion-MNIST\n",
    "##### Entrainement de 2 modèles simples\n",
    "- Modèle normal, entrainé sur Fashion-MNIST clean\n",
    "- Modèle *adversarial*, adv. training --> génère adv. examples, mix clean+adv, modèle robuste\n",
    "\n",
    "![](clean_adv_diff-Fashion-MNIST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dfef8a",
   "metadata": {},
   "source": [
    "#### Analyse des résultats\n",
    "- \"Base\" model\n",
    "    - Clean accuracy = 88%\n",
    "    - Adv accuracy = 0.09-0.26% (perf détruites par FGSM)\n",
    "- \"adv\" model\n",
    "    - Clean accuracy = 85-86% (légère baisse)\n",
    "    - Adv accuracy = 78-85% (robuste ++)\n",
    "![](comparaison_performances-Fashion-MNIST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfbe477",
   "metadata": {},
   "source": [
    "### CIFAR-10\n",
    "##### Entrainement de 2 modèles simples\n",
    "- Modèle normal, entrainé sur CIFAR-10 clean\n",
    "- Modèle *adversarial*, adv. training --> génère adv. examples, mix clean+adv, modèle robuste\n",
    "\n",
    "![](clean_adv_diff_CIFAR-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35020ff4",
   "metadata": {},
   "source": [
    "#### Analyse des résultats\n",
    "- \"Base\" model\n",
    "    - Clean accuracy = 56-58%\n",
    "    - Adv accuracy = 0.23-44% (perf détruites par FGSM)\n",
    "- \"adv\" model\n",
    "    - Clean accuracy = 45% (légère baisse)\n",
    "    - Adv accuracy = 54% (robuste ++)\n",
    "![](comparaison_performances_CIFAR-10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12114f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eaf36d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cell_type": "markdown",
 "id": "impact-epsilon",
 "metadata": {},
 "source": [
  "## Impact du paramètre ε (epsilon) sur la robustesse du modèle\n",
  "\n",
  "### Rôle du paramètre ε\n",
  "\n",
  "Le paramètre ε (epsilon) contrôle l’amplitude maximale de la perturbation appliquée aux images lors de la génération d’exemples adversariaux avec l’attaque **FGSM (Fast Gradient Sign Method)**.\n",
  "\n",
  "Dans FGSM, une perturbation est ajoutée à l’image d’entrée dans la direction du gradient de la fonction de perte par rapport aux pixels :\n",
  "\n",
  "$$\n",
  "x_{adv} = x + \\varepsilon \\cdot \\text{sign}(\\nabla_x L(x, y))\n",
  "$$\n",
  "\n",
  "Un ε faible correspond à une perturbation quasi imperceptible à l’œil humain, tandis qu’un ε plus élevé peut fortement altérer l’image mais augmente l’efficacité de l’attaque adversariale.\n",
  "\n",
  "---\n",
  "\n",
  "### Influence de ε sur les modèles entraînés classiquement\n",
  "\n",
  "Les modèles entraînés uniquement sur des données *clean* se révèlent extrêmement sensibles à la valeur de ε.\n",
  "\n",
  "- Sur **MNIST** et **Fashion-MNIST**, une augmentation modérée de ε suffit à faire chuter drastiquement la précision adversariale, passant de valeurs élevées à des performances proches de zéro.\n",
  "- Sur **CIFAR-10**, cette sensibilité est encore plus marquée en raison de la complexité visuelle des images (textures, couleurs, détails fins).\n",
  "\n",
  "Cette vulnérabilité s’explique par le fait que les modèles standards apprennent des frontières de décision très locales, fortement dépendantes de variations au niveau des pixels, ce qui les rend particulièrement sensibles à de petites perturbations dirigées.\n",
  "\n",
  "---\n",
  "\n",
  "### Impact de ε dans le cadre de l’entraînement adversarial\n",
  "\n",
  "Dans l’entraînement adversarial, ε joue un rôle central dans le compromis entre **robustesse aux attaques** et **performance sur données propres (clean accuracy)**.\n",
  "\n",
  "- **ε trop faible** : les perturbations générées sont peu informatives, ce qui limite le gain en robustesse.\n",
  "- **ε modéré** : le modèle apprend des représentations plus stables, avec une amélioration notable de la précision adversariale et une baisse modérée de la précision clean.\n",
  "- **ε trop élevé** : les exemples adversariaux deviennent trop éloignés des données originales, ce qui dégrade la performance sur données propres et la généralisation.\n",
  "\n",
  "Dans nos expériences, les valeurs choisies (ε = 0.1 pour **Fashion-MNIST** et ε = 0.03 pour **CIFAR-10**) correspondent à un compromis empirique permettant d’améliorer significativement la robustesse sans perte excessive de précision sur données propres.\n",
  "\n",
  "---\n",
  "\n",
  "### Dépendance de ε au dataset\n",
  "\n",
  "- **Datasets simples (MNIST, Fashion-MNIST)** : les images en niveaux de gris tolèrent des valeurs de ε relativement plus élevées sans altérer la sémantique.\n",
  "- **Datasets complexes (CIFAR-10)** : les images RGB sont plus sensibles aux perturbations, nécessitant des valeurs de ε plus faibles.\n",
  "\n",
  "Cela montre que ε ne peut pas être choisi de manière universelle et doit être adapté à la complexité visuelle et aux caractéristiques du dataset.\n",
  "\n",
  "---\n",
  "\n",
  "### Limites et perspectives\n",
  "\n",
  "La robustesse acquise pour une valeur donnée de ε reste spécifique à cette valeur. Un modèle entraîné avec un ε fixe peut rester vulnérable à des attaques utilisant d’autres amplitudes ou des méthodes plus puissantes comme **PGD**.\n",
  "\n",
  "Des pistes d’amélioration incluent l’utilisation d’un ε variable durant l’entraînement, l’évaluation sur une plage de valeurs de ε et la combinaison avec des attaques multi-étapes.\n",
  "\n",
  "---\n",
  "\n",
  "### Conclusion\n",
  "\n",
  "Le paramètre ε constitue un hyperparamètre clé de l’entraînement adversarial. Il détermine l’intensité des perturbations, influence directement la robustesse du modèle et conditionne le compromis fondamental entre précision sur données propres et résistance aux attaques adversariales.\n"
 ]
}

