{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "impact-epsilon",
   "metadata": {},
   "source": [
    "## Impact du paramètre ε (epsilon) sur la robustesse du modèle\n",
    "\n",
    "### Rôle du paramètre ε\n",
    "\n",
    "Le paramètre ε (epsilon) contrôle l’amplitude maximale de la perturbation appliquée aux images lors de la génération d’exemples adversariaux avec l’attaque **FGSM (Fast Gradient Sign Method)**.\n",
    "\n",
    "Dans FGSM, une perturbation est ajoutée à l’image d’entrée dans la direction du gradient de la fonction de perte par rapport aux pixels :\n",
    "\n",
    "$$\n",
    "x_{adv} = x + \\varepsilon \\cdot \\text{sign}(\\nabla_x L(x, y))\n",
    "$$\n",
    "\n",
    "Un ε faible correspond à une perturbation quasi imperceptible à l’œil humain, tandis qu’un ε plus élevé peut fortement altérer l’image mais augmente l’efficacité de l’attaque adversariale.\n",
    "\n",
    "---\n",
    "\n",
    "### Influence de ε sur les modèles entraînés classiquement\n",
    "\n",
    "Les modèles entraînés uniquement sur des données *clean* se révèlent extrêmement sensibles à la valeur de ε.\n",
    "\n",
    "- Sur **MNIST** et **Fashion-MNIST**, une augmentation modérée de ε suffit à faire chuter drastiquement la précision adversariale, passant de valeurs élevées à des performances proches de zéro.\n",
    "- Sur **CIFAR-10**, cette sensibilité est encore plus marquée en raison de la complexité visuelle des images (textures, couleurs, détails fins).\n",
    "\n",
    "Cette vulnérabilité s’explique par le fait que les modèles standards apprennent des frontières de décision très locales, fortement dépendantes de variations au niveau des pixels, ce qui les rend particulièrement sensibles à de petites perturbations dirigées.\n",
    "\n",
    "---\n",
    "\n",
    "### Impact de ε dans le cadre de l’entraînement adversarial\n",
    "\n",
    "Dans l’entraînement adversarial, ε joue un rôle central dans le compromis entre **robustesse aux attaques** et **performance sur données propres (clean accuracy)**.\n",
    "\n",
    "- **ε trop faible** : les perturbations générées sont peu informatives, ce qui limite le gain en robustesse.\n",
    "- **ε modéré** : le modèle apprend des représentations plus stables, avec une amélioration notable de la précision adversariale et une baisse modérée de la précision clean.\n",
    "- **ε trop élevé** : les exemples adversariaux deviennent trop éloignés des données originales, ce qui dégrade la performance sur données propres et la généralisation.\n",
    "\n",
    "Dans nos expériences, les valeurs choisies (ε = 0.1 pour **Fashion-MNIST** et ε = 0.03 pour **CIFAR-10**) correspondent à un compromis empirique permettant d’améliorer significativement la robustesse sans perte excessive de précision sur données propres.\n",
    "\n",
    "---\n",
    "\n",
    "### Dépendance de ε au dataset\n",
    "\n",
    "- **Datasets simples (MNIST, Fashion-MNIST)** : les images en niveaux de gris tolèrent des valeurs de ε relativement plus élevées sans altérer la sémantique.\n",
    "- **Datasets complexes (CIFAR-10)** : les images RGB sont plus sensibles aux perturbations, nécessitant des valeurs de ε plus faibles.\n",
    "\n",
    "Cela montre que ε ne peut pas être choisi de manière universelle et doit être adapté à la complexité visuelle et aux caractéristiques du dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### Limites et perspectives\n",
    "\n",
    "La robustesse acquise pour une valeur donnée de ε reste spécifique à cette valeur. Un modèle entraîné avec un ε fixe peut rester vulnérable à des attaques utilisant d’autres amplitudes ou des méthodes plus puissantes comme **PGD**.\n",
    "\n",
    "Des pistes d’amélioration incluent l’utilisation d’un ε variable durant l’entraînement, l’évaluation sur une plage de valeurs de ε et la combinaison avec des attaques multi-étapes.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Le paramètre ε constitue un hyperparamètre clé de l’entraînement adversarial. Il détermine l’intensité des perturbations, influence directement la robustesse du modèle et conditionne le compromis fondamental entre précision sur données propres et résistance aux attaques adversariales.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
