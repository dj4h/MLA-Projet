{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efec2cdd",
   "metadata": {},
   "source": [
    "# Projet MLA\n",
    "## EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80c890d",
   "metadata": {},
   "source": [
    "!!!!!!!!!! utiliser le gpu !! et si on utilise pas en semaine ça ns déconnecte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96559494",
   "metadata": {},
   "source": [
    "### MNIST\n",
    "\n",
    "#### Entrainement de 2 modèles simples\n",
    "- Modèle normal, entrainé sur MNIST clean\n",
    "- Modèle *adversarial*, adv. training --> génère adv. examples, mix clean+adv, modèle robuste\n",
    "\n",
    "![](clean_adv_diff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a5e7a5",
   "metadata": {},
   "source": [
    "#### Analyse des résultats\n",
    "- \"Base\" model\n",
    "    - Clean accuracy = 98%\n",
    "    - Adv accuracy = 5-20% (perf détruites par FGSM)\n",
    "- \"adv\" model\n",
    "    - Clean accuracy = 96-98% (légère baisse)\n",
    "    - Adv accuracy = 90-93% (robuste ++)\n",
    "![](comparaison_performances.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14d8e5",
   "metadata": {},
   "source": [
    "## Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec3011b",
   "metadata": {},
   "source": [
    "## Paramètres à faire varier pour étudier la robustesse et la fiabilité\n",
    "|**Paramètre |Effet attendu  / Ce que ça teste** |\n",
    "|----------|----------------------------------|\n",
    "|**Epsilon (FGSM / adversarial attack)**|Intensité de perturbation → robustesse du modèle aux attaques|\n",
    "|**Alpha dans adversarial training**|Poids entre loss clean et loss adversarial → impact sur trade-off clean vs robust accuracy|\n",
    "|**Batch size**|Influence la convergence et stabilité de l’entraînement|\n",
    "|**Learning rate**|Trop grand → modèle instable, trop petit → apprentissage lent|\n",
    "|**Nombre d’époques**|Vérifier si le modèle sur-entraîne ou sous-entraîne|\n",
    "|**Architecture du modèle (nombres de filtres, couches, FC)**|\tTeste si plus complexe = meilleure robustesse ou sur-apprentissage\n",
    "|**Type d’attaque (FGSM, PGD, DeepFool, etc.)**|Teste la robustesse face à différentes perturbations|\n",
    "|**Ajout de bruit aléatoire / transformations**|Data augmentation pour tester généralisation et fiabilité sur images légèrement modifiées|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7878a2af",
   "metadata": {},
   "source": [
    "## Évaluation du modèle sur d'autres base de données\n",
    "\n",
    "### Pourquoi?\n",
    "Dans la 1ère partie on a pu réimplémenter les étapes de constructions du modèle décrit dans l'article. À partir de la base de donnée MNIST de PyTorch, le modèle a pu être entraîné pour établir la méthode d'entraînement adversarial la plus adaptée au traitement de ces données. \n",
    "Toutefois, nous sommes en doit de nous demander :\n",
    "- Que vaut notre modèle sur une base de donnée alternative?\n",
    "- De quelle manière pourroins nous améliorer les performances de notre modèle en l'entrainant sur d'autres bases de données ?\n",
    "\n",
    "### Bases de données accessibles avec PyTorch\n",
    "- MNIST, sur laquelle est entraîné le modèle\n",
    "- Fashion-MNIST, 28×28 - grayscale\t10 (vêtements)\tTest de généralisation sur données visuelles plus variées que MNIST \n",
    "- CIFAR-10\t32×32, RGB\t10\tImages naturelles, animaux et véhicules\n",
    "- CIFAR-100\t32×32, RGB\t100\tClassification plus fine, test de capacité à gérer plus de classes\n",
    "- SVHN (Street View House Numbers)\t32×32, RGB\t10\tReconnaissance de chiffres dans des contextes réels\n",
    "- MNIST-M\t28×28, RGB\t10\tMNIST modifié avec bruit / background → test de robustesse\n",
    "\n",
    "#### Mention spéciale pour les datasets Kaggle\n",
    "- Custom datasets\tVariable\tVariable\tImages médicales, industrielles, etc.\n",
    "\n",
    "### Critère de choix\n",
    "- **MNIST / Fashion-MNIST**\n",
    "    - Simple, rapide, parfait pour tester un modèle de base\n",
    "- **CIFAR / SVHN**\n",
    "    - Images colorées plus complexes → test de capacité à extraire des features plus fines\n",
    "- **MNIST-M / datasets bruités** \n",
    "    - Tester la robustesse aux perturbations et aux adversarial examples\n",
    "- **Datasets custom**\n",
    "    - Tester la fiabilité dans un contexte réel ou industriel\n",
    "\n",
    "### Adaptation du modèle en fonction des **objectifs** :\n",
    "- Généralisation vs robustesse\n",
    "- Tâche simple vs tâche complexe\n",
    "- Grayscale vs RGB\n",
    "\n",
    "### Quel serait le protocole le plus pertinent pour étudier les limites de notre modèle et améliorer ses performances en l'entrainant sur différentes bases de données?\n",
    "\n",
    "#### Définir les objectifs\n",
    "\n",
    "Avant tout, ce que tu veux mesurer :\n",
    "\n",
    "- Robustesse : comment le modèle résiste aux attaques adversariales ou aux perturbations.\n",
    "- Généralisation : performance sur des données nouvelles / différentes de l’entraînement.\n",
    "- Fiabilité : stabilité des prédictions en présence de bruit, transformations ou variations dans les données.\n",
    "- Performance pure : précision (accuracy), F1-score, etc. sur différentes bases\n",
    "\n",
    "#### Choisir les datasets\n",
    "\n",
    "Sélectionner plusieurs bases de données, avec des caractéristiques variées :\n",
    "\n",
    "- Simples et proches de MNIST : MNIST, Fashion-MNIST → test de base et débogage rapide.\n",
    "- Images colorées / plus complexes : CIFAR-10/100, SVHN → test de généralisation et capacité du modèle à extraire des features complexes.\n",
    "- Variantes bruitées / perturbées : MNIST-M, datasets augmentés → tester robustesse.\n",
    "- Custom / réelles : images médicales, industrielles → tester fiabilité et applicabilité réelle.\n",
    "\n",
    "#### Adapter le modèle\n",
    "\n",
    "- Ajuster entrée et sortie : nombre de canaux (grayscale → RGB), taille d’image, nombre de classes.\n",
    "\n",
    "- Éventuellement modifier l’architecture si le dataset est plus complexe : plus de filtres, couches, dropout, batch normalization.\n",
    "\n",
    "#### Définir les protocoles d’entraînement\n",
    "\n",
    "Pour chaque dataset :\n",
    "1. Mode standard (clean)\n",
    "    - Entraîner le modèle sur données clean.\n",
    "    - Tester sur données clean et adversariales.\n",
    "2. Mode adversarial training\n",
    "    - Générer adversarial examples (FGSM, PGD, etc.)\n",
    "    - Combiner loss clean et loss adversarial (alpha * clean + (1-alpha) * adv)\n",
    "    - Tester sur clean, adversarial, et éventuellement sur bruit/noise.\n",
    "3. Data augmentation / bruit\n",
    "    - Rotation, translation, scaling, bruit gaussien\n",
    "    - Vérifier si la performance et la robustesse s’améliorent.\n",
    "\n",
    "#### Faire varier les bons paramètres\n",
    "Pour chaque dataset, expérimenter avec :\n",
    "\n",
    "|Paramètre|\tObjectif|\n",
    "|-------------|--------------------------------|\n",
    "|Learning rate|Tester stabilité et vitesse d’apprentissage|\n",
    "|Batch size\t|Impact sur convergence et régularisation|\n",
    "|Epsilon (FGSM/PGD)\t|Mesurer robustesse face à perturbations|\n",
    "|Alpha adversarial\t|Trade-off clean vs robust accuracy|\n",
    "|Architecture\t|Nombre de filtres, couches → capacité du modèle|\n",
    "|Nombre d’époques\t|Sur- ou sous-entraînement|\n",
    "|Augmentation / bruit\t|Généralisation et fiabilité|\n",
    "\n",
    "#### Évaluation systématique\n",
    "\n",
    "Pour chaque expérience, mesurer :\n",
    "- Accuracy / Loss sur clean et adversarial\n",
    "- Courbes d’apprentissage (train/test loss vs epochs)\n",
    "- Comparaison graphique des modèles (clean vs adversarial)\n",
    "- Visualisation des adversarial examples et des perturbations\n",
    "- Analyse des erreurs : quelles classes sont les plus fragiles ?\n",
    "\n",
    "#### Comparaison inter-datasets\n",
    "\n",
    "- Observer comment le modèle réagit à différents types de données.\n",
    "- Identifier les datasets ou les conditions qui réduisent les performances.\n",
    "- Tester transfert de connaissances : entraîner sur dataset A, tester sur dataset B → mesure de la généralisation.\n",
    "\n",
    "#### Amélioration itérative\n",
    "\n",
    "À partir des observations :\n",
    "- Ajouter des couches, dropout, batch normalization pour plus de robustesse.\n",
    "- Ajuster alpha, epsilon pour adversarial training optimal.\n",
    "- Ajouter augmentation ou régularisation pour mieux généraliser.\n",
    "- Répéter le protocole sur plusieurs datasets pour confirmer la robustesse et fiabilité."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5375489",
   "metadata": {},
   "source": [
    "### Adaptation du code MNIST à un entrainement multidatasets\n",
    "\n",
    "- Adaptation de fgsm_attack : \n",
    "    - Détacher le tenseur et en créer un nouveau avec grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d344c5a",
   "metadata": {},
   "source": [
    "### Fashion-MNIST\n",
    "##### Entrainement de 2 modèles simples\n",
    "- Modèle normal, entrainé sur Fashion-MNIST clean\n",
    "- Modèle *adversarial*, adv. training --> génère adv. examples, mix clean+adv, modèle robuste\n",
    "\n",
    "![](clean_adv_diff-Fashion-MNIST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dfef8a",
   "metadata": {},
   "source": [
    "#### Analyse des résultats\n",
    "- \"Base\" model\n",
    "    - Clean accuracy = 88%\n",
    "    - Adv accuracy = 0.09-0.26% (perf détruites par FGSM)\n",
    "- \"adv\" model\n",
    "    - Clean accuracy = 85-86% (légère baisse)\n",
    "    - Adv accuracy = 78-85% (robuste ++)\n",
    "![](comparaison_performances-Fashion-MNIST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfbe477",
   "metadata": {},
   "source": [
    "### CIFAR-10\n",
    "##### Entrainement de 2 modèles simples\n",
    "- Modèle normal, entrainé sur CIFAR-10 clean\n",
    "- Modèle *adversarial*, adv. training --> génère adv. examples, mix clean+adv, modèle robuste\n",
    "\n",
    "![](clean_adv_diff_CIFAR-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35020ff4",
   "metadata": {},
   "source": [
    "#### Analyse des résultats\n",
    "- \"Base\" model\n",
    "    - Clean accuracy = 56-58%\n",
    "    - Adv accuracy = 0.23-44% (perf détruites par FGSM)\n",
    "- \"adv\" model\n",
    "    - Clean accuracy = 45% (légère baisse)\n",
    "    - Adv accuracy = 54% (robuste ++)\n",
    "![](comparaison_performances_CIFAR-10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c04e177",
   "metadata": {},
   "source": [
    "![](clean_adv_diff.png)\n",
    "\n",
    "![](pgd_attack_MNIST.png)\n",
    "\n",
    "Code couleur pour la différence, directions d’amélioration données par le gradient\n",
    "- rouge : gradient positif (augmenter le pixel)\n",
    "- bleu : gradient négatif (diminuer le pixel)\n",
    "- blanc : gradient nul\n",
    "\n",
    "La visualisation du gradient met en évidence les régions de l’image les plus sensibles pour le modèle ainsi que les directions dans lesquelles les pixels doivent être modifiés pour augmenter la perte.\n",
    "Les zones rouges et bleues, concentrées sur les traits du chiffre, montrent que le réseau repose sur des motifs locaux précis, tandis que les zones blanches indiquent des pixels peu informatifs.\n",
    "Ces cartes de gradient fournissent une interprétation directe du fonctionnement des attaques adversariales comme FGSM et PGD.\n",
    "\n",
    "FGSM applique une perturbation uniforme à partir du signe du gradient alors que PGD affiine la perturbation étape par étape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eaf36d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
